{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feedforward Neural Network Toolkit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Callable, Dict, Iterable, Iterator, Tuple\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "LABEL_MAP: Dict[int, str] = {0: 'a', 1: 'e', 2: 'g', 3: 'i', 4: 'l', 5: 'n', 6: 'o', 7: 'r', 8: 't', 9: 'u'}\n",
        "OCR_ROOT = Path('图像分类-dataset')\n",
        "REG_ROOT = Path('回归-dataset')\n",
        "\n",
        "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
        "    x = np.clip(x, -60.0, 60.0)\n",
        "    return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "def relu(x: np.ndarray) -> np.ndarray:\n",
        "    return np.maximum(0.0, x)\n",
        "\n",
        "def softmax(z: np.ndarray) -> np.ndarray:\n",
        "    shifted = z - np.max(z, axis=1, keepdims=True)\n",
        "    exp = np.exp(shifted)\n",
        "    return exp / np.sum(exp, axis=1, keepdims=True)\n",
        "\n",
        "def cross_entropy(probs: np.ndarray, targets: np.ndarray) -> float:\n",
        "    probs = np.clip(probs, 1e-12, 1.0)\n",
        "    return -float(np.sum(targets * np.log(probs)) / len(targets))\n",
        "\n",
        "def mse(preds: np.ndarray, targets: np.ndarray) -> float:\n",
        "    diff = preds - targets\n",
        "    return float(0.5 * np.mean(diff * diff))\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class ActivationSpec:\n",
        "    forward: Callable[[np.ndarray], np.ndarray]\n",
        "    backward: Callable[[np.ndarray, np.ndarray], np.ndarray]\n",
        "\n",
        "def _sigmoid_grad(_: np.ndarray, activated: np.ndarray) -> np.ndarray:\n",
        "    return activated * (1.0 - activated)\n",
        "\n",
        "def _relu_grad(pre_activation: np.ndarray, _: np.ndarray) -> np.ndarray:\n",
        "    return (pre_activation > 0.0).astype(float)\n",
        "\n",
        "ACTIVATIONS: Dict[str, ActivationSpec] = {\n",
        "    'sigmoid': ActivationSpec(sigmoid, _sigmoid_grad),\n",
        "    'relu': ActivationSpec(relu, _relu_grad),\n",
        "}\n",
        "\n",
        "class FeedForwardNetwork:\n",
        "    def __init__(self, layer_sizes: Iterable[int], activation: str = 'sigmoid', seed: int = 0):\n",
        "        self.layer_sizes = tuple(layer_sizes)\n",
        "        if len(self.layer_sizes) < 2:\n",
        "            raise ValueError('layer_sizes must include input and output dimensions')\n",
        "        try:\n",
        "            self.activation = ACTIVATIONS[activation]\n",
        "        except KeyError as exc:\n",
        "            raise ValueError(f'Unsupported activation: {activation}') from exc\n",
        "        rng = np.random.default_rng(seed)\n",
        "        self.params = [\n",
        "            [rng.standard_normal((inp, out)) * np.sqrt(2.0 / (inp + out)), np.zeros(out)]\n",
        "            for inp, out in zip(self.layer_sizes[:-1], self.layer_sizes[1:])\n",
        "        ]\n",
        "\n",
        "    def forward(self, X: np.ndarray, return_cache: bool = False):\n",
        "        activations = [X]\n",
        "        pre_acts = []\n",
        "        for idx, (W, b) in enumerate(self.params):\n",
        "            z = activations[-1] @ W + b\n",
        "            pre_acts.append(z)\n",
        "            if idx == len(self.params) - 1:\n",
        "                a = z\n",
        "            else:\n",
        "                a = self.activation.forward(z)\n",
        "            activations.append(a)\n",
        "        if return_cache:\n",
        "            return activations[-1], (activations, pre_acts)\n",
        "        return activations[-1]\n",
        "\n",
        "    def backward(self, grad_output: np.ndarray, cache: Tuple[Iterable[np.ndarray], Iterable[np.ndarray]]):\n",
        "        activations, pre_acts = cache\n",
        "        grads = []\n",
        "        delta = grad_output\n",
        "        batch_size = activations[0].shape[0]\n",
        "        for idx in reversed(range(len(self.params))):\n",
        "            a_prev = activations[idx]\n",
        "            W, _ = self.params[idx]\n",
        "            grads.insert(0, (a_prev.T @ delta / batch_size, np.mean(delta, axis=0)))\n",
        "            if idx:\n",
        "                delta = delta @ W.T\n",
        "                delta *= self.activation.backward(pre_acts[idx - 1], activations[idx])\n",
        "        return grads\n",
        "\n",
        "    def apply_grads(self, grads, lr: float):\n",
        "        for (dW, db), param in zip(grads, self.params):\n",
        "            param[0] -= lr * dW\n",
        "            param[1] -= lr * db\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        return np.argmax(self.forward(X), axis=1)\n",
        "\n",
        "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
        "        return softmax(self.forward(X))\n",
        "\n",
        "    def predict_regression(self, X: np.ndarray) -> np.ndarray:\n",
        "        return self.forward(X)\n",
        "\n",
        "def _create_encoder() -> OneHotEncoder:\n",
        "    try:\n",
        "        return OneHotEncoder(sparse_output=False)\n",
        "    except TypeError:\n",
        "        return OneHotEncoder(sparse=False)\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class ClassificationData:\n",
        "    X_train: np.ndarray\n",
        "    y_train: np.ndarray\n",
        "    y_train_onehot: np.ndarray\n",
        "    X_test: np.ndarray\n",
        "    y_test: np.ndarray\n",
        "    encoder: OneHotEncoder\n",
        "\n",
        "def load_ocr_dataset(dataset_size: int, test_ratio: float = 0.2) -> ClassificationData:\n",
        "    df = pd.read_csv(OCR_ROOT / f'{dataset_size}Train.csv', header=None)\n",
        "    y = df.iloc[:, 0].to_numpy(dtype=int)\n",
        "    X = df.iloc[:, 1:].to_numpy(dtype=np.float32)\n",
        "    if X.max() > 1.0:\n",
        "        X = X / 255.0\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=test_ratio, stratify=y, random_state=0\n",
        "    )\n",
        "    encoder = _create_encoder()\n",
        "    y_train_onehot = encoder.fit_transform(y_train.reshape(-1, 1))\n",
        "    y_test_onehot = encoder.transform(y_test.reshape(-1, 1))\n",
        "    return ClassificationData(X_train, y_train, y_train_onehot, X_test, y_test, encoder)\n",
        "\n",
        "def iterate_minibatches(*arrays: np.ndarray, batch_size: int, rng: np.random.Generator) -> Iterator[Tuple[np.ndarray, ...]]:\n",
        "    if not arrays:\n",
        "        return\n",
        "    size = arrays[0].shape[0]\n",
        "    order = rng.permutation(size)\n",
        "    for start in range(0, size, batch_size):\n",
        "        idx = order[start:start + batch_size]\n",
        "        yield tuple(arr[idx] for arr in arrays)\n",
        "\n",
        "@dataclass\n",
        "class ClassifierResult:\n",
        "    network: FeedForwardNetwork\n",
        "    losses: list\n",
        "    accuracy: float\n",
        "    probs: np.ndarray\n",
        "    preds: np.ndarray\n",
        "    data: ClassificationData\n",
        "\n",
        "def train_classifier(\n",
        "    dataset_size: int,\n",
        "    activation: str = 'sigmoid',\n",
        "    hidden_layers: Tuple[int, ...] = (128, 62),\n",
        "    lr: float = 0.5,\n",
        "    epochs: int = 50,\n",
        "    batch_size: int = 64,\n",
        "    seed: int = 0,\n",
        ") -> ClassifierResult:\n",
        "    data = load_ocr_dataset(dataset_size)\n",
        "    layer_sizes = (data.X_train.shape[1], *hidden_layers, data.y_train_onehot.shape[1])\n",
        "    net = FeedForwardNetwork(layer_sizes, activation=activation, seed=seed)\n",
        "    rng = np.random.default_rng(seed)\n",
        "    losses = []\n",
        "    for _ in range(epochs):\n",
        "        for xb, yb in iterate_minibatches(data.X_train, data.y_train_onehot, batch_size=batch_size, rng=rng):\n",
        "            logits, cache = net.forward(xb, return_cache=True)\n",
        "            probs = softmax(logits)\n",
        "            grad_logits = (probs - yb) / len(xb)\n",
        "            grads = net.backward(grad_logits, cache)\n",
        "            net.apply_grads(grads, lr)\n",
        "        train_logits = net.forward(data.X_train)\n",
        "        losses.append(cross_entropy(softmax(train_logits), data.y_train_onehot))\n",
        "    test_probs = net.predict_proba(data.X_test)\n",
        "    test_preds = np.argmax(test_probs, axis=1)\n",
        "    accuracy = float(np.mean(test_preds == data.y_test))\n",
        "    return ClassifierResult(net, losses, accuracy, test_probs, test_preds, data)\n",
        "\n",
        "def plot_classifier_losses(results: Dict[str, ClassifierResult]):\n",
        "    plt.figure(figsize=(7, 4))\n",
        "    for name, outcome in results.items():\n",
        "        epochs = range(1, len(outcome.losses) + 1)\n",
        "        plt.plot(list(epochs), outcome.losses, label=name)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Cross-Entropy')\n",
        "    plt.title('Training Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True, linewidth=0.3)\n",
        "    plt.tight_layout()\n",
        "\n",
        "def show_classifier_examples(result: ClassifierResult, count: int = 5, seed: int = 0):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    indices = rng.choice(len(result.data.X_test), size=count, replace=False)\n",
        "    fig, axes = plt.subplots(1, count, figsize=(3 * count, 3))\n",
        "    for ax, idx in zip(axes, indices):\n",
        "        image = result.data.X_test[idx].reshape(16, 8)\n",
        "        true_label = LABEL_MAP[result.data.y_test[idx]]\n",
        "        pred_label = LABEL_MAP[result.preds[idx]]\n",
        "        ax.imshow(1.0 - image, cmap='gray', vmin=0, vmax=1)\n",
        "        ax.set_title(f'T:{true_label} P:{pred_label}')\n",
        "        ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class NormalizationStats:\n",
        "    x_mean: float\n",
        "    x_std: float\n",
        "    y_mean: float\n",
        "    y_std: float\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class RegressionSplit:\n",
        "    X: np.ndarray\n",
        "    y: np.ndarray\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class RegressionDataset:\n",
        "    train: RegressionSplit\n",
        "    id: RegressionSplit\n",
        "    ood: RegressionSplit\n",
        "    raw: Dict[str, RegressionSplit]\n",
        "    stats: NormalizationStats\n",
        "\n",
        "def prepare_regression_data(train_size: int = 1800, seed: int = 42) -> RegressionDataset:\n",
        "    train_df = pd.read_csv(REG_ROOT / 'data_train.csv', header=None, skiprows=1)\n",
        "    valid_df = pd.read_csv(REG_ROOT / 'data_valid.csv', header=None, skiprows=1)\n",
        "    test_df = pd.read_csv(REG_ROOT / 'data_test.csv', header=None, skiprows=1)\n",
        "    shuffled = train_df.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
        "    X_all = shuffled.iloc[:, 0].to_numpy(float).reshape(-1, 1)\n",
        "    y_all = shuffled.iloc[:, 1].to_numpy(float).reshape(-1, 1)\n",
        "    X_train, y_train = X_all[:train_size], y_all[:train_size]\n",
        "    X_id, y_id = X_all[train_size:], y_all[train_size:]\n",
        "    X_ood = np.concatenate(\n",
        "        [valid_df.iloc[:, 0].to_numpy(float), test_df.iloc[:, 0].to_numpy(float)]\n",
        "    ).reshape(-1, 1)\n",
        "    y_ood = np.concatenate(\n",
        "        [valid_df.iloc[:, 1].to_numpy(float), test_df.iloc[:, 1].to_numpy(float)]\n",
        "    ).reshape(-1, 1)\n",
        "    x_mean = float(X_train.mean())\n",
        "    x_std = max(float(X_train.std()), 1e-8)\n",
        "    y_mean = float(y_train.mean())\n",
        "    y_std = max(float(y_train.std()), 1e-8)\n",
        "    stats = NormalizationStats(x_mean, x_std, y_mean, y_std)\n",
        "    def norm_x(x: np.ndarray) -> np.ndarray:\n",
        "        return (x - stats.x_mean) / stats.x_std\n",
        "    def norm_y(y: np.ndarray) -> np.ndarray:\n",
        "        return (y - stats.y_mean) / stats.y_std\n",
        "    train_split = RegressionSplit(norm_x(X_train), norm_y(y_train))\n",
        "    id_split = RegressionSplit(norm_x(X_id), norm_y(y_id))\n",
        "    ood_split = RegressionSplit(norm_x(X_ood), norm_y(y_ood))\n",
        "    raw = {\n",
        "        'train': RegressionSplit(X_train, y_train),\n",
        "        'id': RegressionSplit(X_id, y_id),\n",
        "        'ood': RegressionSplit(X_ood, y_ood),\n",
        "    }\n",
        "    return RegressionDataset(train_split, id_split, ood_split, raw, stats)\n",
        "\n",
        "def denormalize(y: np.ndarray, stats: NormalizationStats) -> np.ndarray:\n",
        "    return y * stats.y_std + stats.y_mean\n",
        "\n",
        "@dataclass\n",
        "class RegressionResult:\n",
        "    network: FeedForwardNetwork\n",
        "    data: RegressionDataset\n",
        "    history: Dict[str, list]\n",
        "\n",
        "def train_regressor(\n",
        "    train_size: int = 1800,\n",
        "    hidden_layers: Tuple[int, ...] = (64, 32),\n",
        "    lr: float = 1e-3,\n",
        "    epochs: int = 20000,\n",
        "    log_every: int = 100,\n",
        "    seed: int = 42,\n",
        ") -> RegressionResult:\n",
        "    data = prepare_regression_data(train_size=train_size, seed=seed)\n",
        "    net = FeedForwardNetwork((1, *hidden_layers, 1), activation='sigmoid', seed=seed)\n",
        "    history = {'steps': [], 'train': [], 'id': [], 'ood': []}\n",
        "    X_train, y_train = data.train.X, data.train.y\n",
        "    for step in range(1, epochs + 1):\n",
        "        preds, cache = net.forward(X_train, return_cache=True)\n",
        "        grad = (preds - y_train) / len(X_train)\n",
        "        grads = net.backward(grad, cache)\n",
        "        net.apply_grads(grads, lr)\n",
        "        if step % log_every == 0 or step == epochs:\n",
        "            history['steps'].append(step)\n",
        "            history['train'].append(mse(net.predict_regression(X_train), y_train))\n",
        "            history['id'].append(mse(net.predict_regression(data.id.X), data.id.y))\n",
        "            history['ood'].append(mse(net.predict_regression(data.ood.X), data.ood.y))\n",
        "    return RegressionResult(net, data, history)\n",
        "\n",
        "def plot_regression_fit(result: RegressionResult, title: str = 'FFNN Regression', grid=(-4.0, 6.0, 300)):\n",
        "    stats = result.data.stats\n",
        "    net = result.network\n",
        "    x_grid = np.linspace(grid[0], grid[1], grid[2]).reshape(-1, 1)\n",
        "    x_norm = (x_grid - stats.x_mean) / stats.x_std\n",
        "    y_grid = denormalize(net.predict_regression(x_norm), stats)\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    for key, color in [('train', 'tab:blue'), ('id', 'tab:orange'), ('ood', 'tab:green')]:\n",
        "        split = result.data.raw[key]\n",
        "        plt.scatter(split.X, split.y, s=10, label=key.upper(), color=color)\n",
        "    plt.plot(x_grid, y_grid, color='tab:red', linewidth=2, label='prediction')\n",
        "    plt.axvline(x=2.0, color='black', linestyle='--')\n",
        "    plt.legend()\n",
        "    plt.xlabel('X')\n",
        "    plt.ylabel('Y')\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "\n",
        "def plot_regression_history(result: RegressionResult):\n",
        "    hist = result.history\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.plot(hist['steps'], hist['train'], label='train')\n",
        "    plt.plot(hist['steps'], hist['id'], label='id test')\n",
        "    plt.plot(hist['steps'], hist['ood'], label='ood test')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('MSE')\n",
        "    plt.title('Regression Loss History')\n",
        "    plt.legend()\n",
        "    plt.grid(True, linewidth=0.3)\n",
        "    plt.tight_layout()\n",
        "\n",
        "def computational_graph_values(x: Iterable[float]):\n",
        "    x1, x2, x3 = x\n",
        "    z1 = 2 * x1 + x2\n",
        "    z2 = x1 * 3 * x3\n",
        "    z3 = -x3 * 2 * x2\n",
        "    u1 = np.sin(z1)\n",
        "    u2 = 6 * x3 + 2 * z2\n",
        "    u3 = 2 * z1 + z3\n",
        "    v1 = u1 + np.cos(u3)\n",
        "    v2 = np.sin(-u2)\n",
        "    v3 = u1 * u3\n",
        "    y1 = v1 ** 2 + v2 ** 3\n",
        "    y2 = v2 * v3\n",
        "    J_yv = np.array([[2 * v1, 3 * v2 ** 2, 0.0], [0.0, v3, v2]])\n",
        "    J_vu = np.array([[1.0, 0.0, -np.sin(u3)], [0.0, -np.cos(-u2), 0.0], [u3, 0.0, u1]])\n",
        "    return {\n",
        "        'y': np.array([y1, y2], dtype=float),\n",
        "        'J_yv': J_yv,\n",
        "        'J_vu': J_vu,\n",
        "        'J_yu': J_yv @ J_vu,\n",
        "    }\n",
        "\n",
        "def numeric_jacobian(func, x: Iterable[float], h: float = 1e-5) -> np.ndarray:\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    y0 = func(x)\n",
        "    m = len(y0)\n",
        "    n = len(x)\n",
        "    J = np.zeros((m, n))\n",
        "    for j in range(n):\n",
        "        dx = np.zeros_like(x)\n",
        "        dx[j] = h\n",
        "        J[:, j] = (func(x + dx) - func(x - dx)) / (2 * h)\n",
        "    return J\n",
        "\n",
        "def verify_computational_graph(x: Iterable[float], h: float = 1e-5):\n",
        "    values = computational_graph_values(x)\n",
        "    numeric = numeric_jacobian(lambda inp: computational_graph_values(inp)['y'], x, h)\n",
        "    values['J_numeric'] = numeric\n",
        "    return values\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}