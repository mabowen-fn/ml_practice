{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feedforward Neural Network Toolkit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.utils import shuffle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "LABEL_MAP = {0: 'a', 1: 'e', 2: 'g', 3: 'i', 4: 'l', 5: 'n', 6: 'o', 7: 'r', 8: 't', 9: 'u'}\n",
        "OCR_ROOT = Path('图像分类-dataset')\n",
        "REG_ROOT = Path('回归-dataset')\n",
        "\n",
        "def sigmoid(x):\n",
        "    x = np.clip(x, -60, 60)\n",
        "    return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "def sigmoid_grad(a):\n",
        "    return a * (1.0 - a)\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0.0, x)\n",
        "\n",
        "def relu_grad(z):\n",
        "    return (z > 0).astype(float)\n",
        "\n",
        "def softmax(z):\n",
        "    shifted = z - np.max(z, axis=1, keepdims=True)\n",
        "    exp = np.exp(shifted)\n",
        "    return exp / np.sum(exp, axis=1, keepdims=True)\n",
        "\n",
        "def cross_entropy(probs, targets):\n",
        "    probs = np.clip(probs, 1e-12, 1.0)\n",
        "    return -np.sum(targets * np.log(probs)) / len(targets)\n",
        "\n",
        "def mse(preds, targets):\n",
        "    diff = preds - targets\n",
        "    return 0.5 * np.mean(diff * diff)\n",
        "\n",
        "class FeedForwardNetwork:\n",
        "    def __init__(self, layer_sizes, activation='sigmoid', seed=0):\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.activation = activation\n",
        "        rng = np.random.default_rng(seed)\n",
        "        self.params = [\n",
        "            [rng.standard_normal((m, n)) * np.sqrt(2.0 / (m + n)), np.zeros(n)]\n",
        "            for m, n in zip(layer_sizes[:-1], layer_sizes[1:])\n",
        "        ]\n",
        "\n",
        "    def _activate(self, z):\n",
        "        if self.activation == 'sigmoid':\n",
        "            return sigmoid(z)\n",
        "        if self.activation == 'relu':\n",
        "            return relu(z)\n",
        "        raise ValueError('Unsupported activation')\n",
        "\n",
        "    def _activate_grad(self, z, a):\n",
        "        if self.activation == 'sigmoid':\n",
        "            return sigmoid_grad(a)\n",
        "        if self.activation == 'relu':\n",
        "            return relu_grad(z)\n",
        "        raise ValueError('Unsupported activation')\n",
        "\n",
        "    def forward(self, X, return_cache=False):\n",
        "        activations = [X]\n",
        "        pre_acts = []\n",
        "        for i, (W, b) in enumerate(self.params):\n",
        "            z = activations[-1] @ W + b\n",
        "            pre_acts.append(z)\n",
        "            if i == len(self.params) - 1:\n",
        "                a = z\n",
        "            else:\n",
        "                a = self._activate(z)\n",
        "            activations.append(a)\n",
        "        if return_cache:\n",
        "            return activations[-1], (activations, pre_acts)\n",
        "        return activations[-1]\n",
        "\n",
        "    def backward(self, grad_output, cache):\n",
        "        activations, pre_acts = cache\n",
        "        grads = []\n",
        "        delta = grad_output\n",
        "        m = activations[0].shape[0]\n",
        "        for i in reversed(range(len(self.params))):\n",
        "            a_prev = activations[i]\n",
        "            W, _ = self.params[i]\n",
        "            grads.insert(0, (a_prev.T @ delta / m, np.mean(delta, axis=0)))\n",
        "            if i != 0:\n",
        "                delta = delta @ W.T\n",
        "                delta *= self._activate_grad(pre_acts[i - 1], activations[i])\n",
        "        return grads\n",
        "\n",
        "    def apply_grads(self, grads, lr):\n",
        "        for (dW, db), param in zip(grads, self.params):\n",
        "            param[0] -= lr * dW\n",
        "            param[1] -= lr * db\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.argmax(self.forward(X), axis=1)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        return softmax(self.forward(X))\n",
        "\n",
        "    def predict_regression(self, X):\n",
        "        return self.forward(X)\n",
        "\n",
        "def load_ocr_dataset(dataset_size, test_ratio=0.2):\n",
        "    path = OCR_ROOT / f'{dataset_size}Train.csv'\n",
        "    df = pd.read_csv(path, header=None)\n",
        "    y = df.iloc[:, 0].to_numpy(dtype=int)\n",
        "    X = df.iloc[:, 1:].to_numpy(dtype=np.float32)\n",
        "    if X.max() > 1.0:\n",
        "        X = X / 255.0\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=test_ratio, stratify=y, random_state=0\n",
        "    )\n",
        "    encoder = OneHotEncoder(sparse=False)\n",
        "    y_train_onehot = encoder.fit_transform(y_train.reshape(-1, 1))\n",
        "    y_test_onehot = encoder.transform(y_test.reshape(-1, 1))\n",
        "    return (X_train, y_train, y_train_onehot), (X_test, y_test), encoder\n",
        "\n",
        "def train_classifier(dataset_size, activation='sigmoid', hidden_layers=(128, 62), lr=0.5, epochs=50, batch_size=64, seed=0):\n",
        "    (X_train, y_train, y_train_onehot), (X_test, y_test), encoder = load_ocr_dataset(dataset_size)\n",
        "    layer_sizes = (X_train.shape[1], *hidden_layers, y_train_onehot.shape[1])\n",
        "    net = FeedForwardNetwork(layer_sizes, activation=activation, seed=seed)\n",
        "    losses = []\n",
        "    for epoch in range(epochs):\n",
        "        X_train, y_train_onehot, y_train = shuffle(X_train, y_train_onehot, y_train, random_state=seed + epoch)\n",
        "        for start in range(0, len(X_train), batch_size):\n",
        "            end = start + batch_size\n",
        "            xb = X_train[start:end]\n",
        "            yb = y_train_onehot[start:end]\n",
        "            logits, cache = net.forward(xb, return_cache=True)\n",
        "            probs = softmax(logits)\n",
        "            grad_logits = (probs - yb) / len(xb)\n",
        "            grads = net.backward(grad_logits, cache)\n",
        "            net.apply_grads(grads, lr)\n",
        "        logits = net.forward(X_train)\n",
        "        losses.append(cross_entropy(softmax(logits), y_train_onehot))\n",
        "    test_probs = net.predict_proba(X_test)\n",
        "    test_preds = np.argmax(test_probs, axis=1)\n",
        "    accuracy = np.mean(test_preds == y_test)\n",
        "    return {\n",
        "        'network': net,\n",
        "        'losses': losses,\n",
        "        'test_accuracy': accuracy,\n",
        "        'test_probs': test_probs,\n",
        "        'test_preds': test_preds,\n",
        "        'y_test': y_test,\n",
        "        'X_test': X_test,\n",
        "        'encoder': encoder,\n",
        "        'label_map': LABEL_MAP,\n",
        "    }\n",
        "\n",
        "def plot_classifier_losses(results):\n",
        "    plt.figure(figsize=(7, 4))\n",
        "    for name, outcome in results.items():\n",
        "        plt.plot(range(1, len(outcome['losses']) + 1), outcome['losses'], label=name)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Cross-Entropy')\n",
        "    plt.title('Training Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True, linewidth=0.3)\n",
        "    plt.tight_layout()\n",
        "\n",
        "def show_classifier_examples(result, count=5, seed=0):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    indices = rng.choice(len(result['X_test']), size=count, replace=False)\n",
        "    fig, axes = plt.subplots(1, count, figsize=(3 * count, 3))\n",
        "    for ax, idx in zip(axes, indices):\n",
        "        image = result['X_test'][idx].reshape(16, 8)\n",
        "        true_label = result['label_map'][result['y_test'][idx]]\n",
        "        pred_label = result['label_map'][result['test_preds'][idx]]\n",
        "        ax.imshow(1.0 - image, cmap='gray', vmin=0, vmax=1)\n",
        "        ax.set_title(f'T:{true_label} P:{pred_label}')\n",
        "        ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "\n",
        "def prepare_regression_data(train_size=1800, seed=42):\n",
        "    train_df = pd.read_csv(REG_ROOT / 'data_train.csv', header=None, skiprows=1)\n",
        "    valid_df = pd.read_csv(REG_ROOT / 'data_valid.csv', header=None, skiprows=1)\n",
        "    test_df = pd.read_csv(REG_ROOT / 'data_test.csv', header=None, skiprows=1)\n",
        "    shuffled = train_df.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
        "    X_all = shuffled.iloc[:, 0].to_numpy(float).reshape(-1, 1)\n",
        "    y_all = shuffled.iloc[:, 1].to_numpy(float).reshape(-1, 1)\n",
        "    X_train, y_train = X_all[:train_size], y_all[:train_size]\n",
        "    X_id, y_id = X_all[train_size:], y_all[train_size:]\n",
        "    X_ood = np.concatenate([valid_df.iloc[:, 0].to_numpy(float), test_df.iloc[:, 0].to_numpy(float)])\n",
        "    y_ood = np.concatenate([valid_df.iloc[:, 1].to_numpy(float), test_df.iloc[:, 1].to_numpy(float)])\n",
        "    X_ood = X_ood.reshape(-1, 1)\n",
        "    y_ood = y_ood.reshape(-1, 1)\n",
        "    x_mean = X_train.mean()\n",
        "    x_std = max(float(X_train.std()), 1e-8)\\n",
        "    y_mean = y_train.mean()\\n",
        "    y_std = max(float(y_train.std()), 1e-8)\\n",
        "    def norm_x(x):\n",
        "        return (x - x_mean) / x_std\n",
        "    def norm_y(y):\n",
        "        return (y - y_mean) / y_std\n",
        "    return {\n",
        "        'train': (norm_x(X_train), norm_y(y_train)),\n",
        "        'id': (norm_x(X_id), norm_y(y_id)),\n",
        "        'ood': (norm_x(X_ood), norm_y(y_ood)),\n",
        "        'raw': {\n",
        "            'train': (X_train, y_train),\n",
        "            'id': (X_id, y_id),\n",
        "            'ood': (X_ood, y_ood),\n",
        "        },\n",
        "        'stats': {'x_mean': x_mean, 'x_std': x_std, 'y_mean': y_mean, 'y_std': y_std},\n",
        "    }\n",
        "\n",
        "def denormalize(y, stats):\n",
        "    return y * stats['y_std'] + stats['y_mean']\n",
        "\n",
        "def train_regressor(hidden_layers=(64, 32), lr=1e-3, epochs=20000, log_every=1000, seed=42):\n",
        "    data = prepare_regression_data(seed=seed)\n",
        "    X_train, y_train = data['train']\n",
        "    X_id, y_id = data['id']\n",
        "    X_ood, y_ood = data['ood']\n",
        "    net = FeedForwardNetwork((1, *hidden_layers, 1), activation='sigmoid', seed=seed)\n",
        "    history = {'steps': [], 'train': [], 'id': [], 'ood': []}\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        preds, cache = net.forward(X_train, return_cache=True)\n",
        "        grad = (preds - y_train) / len(X_train)\n",
        "        grads = net.backward(grad, cache)\n",
        "        net.apply_grads(grads, lr)\n",
        "        if epoch % log_every == 0 or epoch == epochs:\n",
        "            history['steps'].append(epoch)\n",
        "            history['train'].append(mse(net.predict_regression(X_train), y_train))\n",
        "            history['id'].append(mse(net.predict_regression(X_id), y_id))\n",
        "            history['ood'].append(mse(net.predict_regression(X_ood), y_ood))\n",
        "    data['history'] = history\n",
        "    data['network'] = net\n",
        "    return data\n",
        "\n",
        "def plot_regression_fit(result, title='FFNN Regression', grid=(-4, 6, 300)):\n",
        "    stats = result['stats']\n",
        "    net = result['network']\n",
        "    x_grid = np.linspace(grid[0], grid[1], grid[2]).reshape(-1, 1)\n",
        "    x_norm = (x_grid - stats['x_mean']) / stats['x_std']\n",
        "    y_grid = denormalize(net.predict_regression(x_norm), stats)\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    for key, color in [('train', 'tab:blue'), ('id', 'tab:orange'), ('ood', 'tab:green')]:\n",
        "        X_raw, y_raw = result['raw'][key]\n",
        "        plt.scatter(X_raw, y_raw, s=10, label=key.upper(), color=color)\n",
        "    plt.plot(x_grid, y_grid, color='tab:red', linewidth=2, label='prediction')\n",
        "    plt.axvline(x=2.0, color='black', linestyle='--')\n",
        "    plt.legend()\n",
        "    plt.xlabel('X')\n",
        "    plt.ylabel('Y')\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "\n",
        "def plot_regression_history(result):\n",
        "    hist = result['history']\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.plot(hist['steps'], hist['train'], label='train')\n",
        "    plt.plot(hist['steps'], hist['id'], label='id test')\n",
        "    plt.plot(hist['steps'], hist['ood'], label='ood test')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('MSE')\n",
        "    plt.title('Regression Loss History')\n",
        "    plt.legend()\n",
        "    plt.grid(True, linewidth=0.3)\n",
        "    plt.tight_layout()\n",
        "\n",
        "def computational_graph_values(x):\n",
        "    x1, x2, x3 = x\n",
        "    z1 = 2 * x1 + x2\n",
        "    z2 = x1 * 3 * x3\n",
        "    z3 = -x3 * 2 * x2\n",
        "    u1 = np.sin(z1)\n",
        "    u2 = 6 * x3 + 2 * z2\n",
        "    u3 = 2 * z1 + z3\n",
        "    v1 = u1 + np.cos(u3)\n",
        "    v2 = np.sin(-u2)\n",
        "    v3 = u1 * u3\n",
        "    y1 = v1 ** 2 + v2 ** 3\n",
        "    y2 = v2 * v3\n",
        "    J_yv = np.array([[2 * v1, 3 * v2 ** 2, 0.0], [0.0, v3, v2]])\n",
        "    J_vu = np.array([[1.0, 0.0, -np.sin(u3)], [0.0, -np.cos(-u2), 0.0], [u3, 0.0, u1]])\n",
        "    return {\n",
        "        'y': np.array([y1, y2], dtype=float),\n",
        "        'J_yv': J_yv,\n",
        "        'J_vu': J_vu,\n",
        "        'J_yu': J_yv @ J_vu,\n",
        "    }\n",
        "\n",
        "def numeric_jacobian(func, x, h=1e-5):\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    y0 = func(x)\n",
        "    m = len(y0)\n",
        "    n = len(x)\n",
        "    J = np.zeros((m, n))\n",
        "    for j in range(n):\n",
        "        dx = np.zeros_like(x)\n",
        "        dx[j] = h\n",
        "        J[:, j] = (func(x + dx) - func(x - dx)) / (2 * h)\n",
        "    return J\n",
        "\n",
        "def verify_computational_graph(x, h=1e-5):\n",
        "    values = computational_graph_values(x)\n",
        "    numeric = numeric_jacobian(lambda inp: computational_graph_values(inp)['y'], x, h)\n",
        "    values['J_numeric'] = numeric\n",
        "    return values\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}