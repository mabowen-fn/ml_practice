{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40a2d99b-427c-411e-b924-dd015d10f211",
   "metadata": {},
   "source": [
    "## 3.1 Computational Graph\n",
    "\n",
    "chain rule ∂y/∂u = ∂y/∂v · ∂v/∂u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35f190c-aa0c-4e35-9e70-e848ee93885e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    # Stable sigmoid: handles large |x| values safely\n",
    "    x = np.clip(x, -500, 500)\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "def numeric_jacobian(func, x, h=1e-5):\n",
    "    \"\"\"Vector-valued func, vector x -> Jacobian matrix\"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    y0 = func(x)\n",
    "    m, n = len(y0), len(x)\n",
    "    J = np.empty((m, n))\n",
    "    for j in range(n):\n",
    "        dx = np.zeros_like(x)\n",
    "        dx[j] = h\n",
    "        J[:, j] = (func(x + dx) - func(x - dx)) / (2 * h)\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0940ca2-ce61-491f-a751-f1bb8b76ab34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompGraph:\n",
    "    def __init__(self):\n",
    "        # storage for intermediate values\n",
    "        self.x1 = self.x2 = self.x3 = None\n",
    "        self.z1 = self.z2 = self.z3 = None\n",
    "        self.u1 = self.u2 = self.u3 = None\n",
    "        self.v1 = self.v2 = self.v3 = None\n",
    "        self.y1 = self.y2 = None\n",
    "\n",
    "    def forward(self, x1, x2, x3):\n",
    "        self.x1, self.x2, self.x3 = x1, x2, x3\n",
    "        self.z1 = 2*x1 + x2\n",
    "        self.z2 = x1 * 3*x3\n",
    "        self.z3 = -x3 * 2*x2\n",
    "        self.u1 = np.sin(self.z1)\n",
    "        self.u2 = 6*x3 + 2*self.z2\n",
    "        self.u3 = 2*self.z1 + self.z3\n",
    "        self.v1 = self.u1 + np.cos(self.u3)\n",
    "        self.v2 = np.sin(-self.u2)\n",
    "        self.v3 = self.u1 * self.u3\n",
    "        self.y1 = self.v1**2 + self.v2**3\n",
    "        self.y2 = self.v2 * self.v3\n",
    "        return np.array([self.y1, self.y2])\n",
    "\n",
    "    # ---------- analytic back-prop ----------\n",
    "    def backward(self):\n",
    "        # dy/dv  (2×3)\n",
    "        dy1_dv1 = 2 * self.v1\n",
    "        dy1_dv2 = 3 * self.v2**2\n",
    "        dy1_dv3 = 0.0\n",
    "        dy2_dv1 = 0.0\n",
    "        dy2_dv2 = self.v3\n",
    "        dy2_dv3 = self.v2\n",
    "        J_yv = np.array([[dy1_dv1, dy1_dv2, dy1_dv3],\n",
    "                         [dy2_dv1, dy2_dv2, dy2_dv3]])\n",
    "\n",
    "        # dv/du  (3×3)\n",
    "        dv1_du1 = 1.0\n",
    "        dv1_du2 = 0.0\n",
    "        dv1_du3 = -np.sin(self.u3)\n",
    "        dv2_du1 = 0.0\n",
    "        dv2_du2 = -np.cos(-self.u2)\n",
    "        dv2_du3 = 0.0\n",
    "        dv3_du1 = self.u3\n",
    "        dv3_du2 = 0.0\n",
    "        dv3_du3 = self.u1\n",
    "        J_vu = np.array([[dv1_du1, dv1_du2, dv1_du3],\n",
    "                         [dv2_du1, dv2_du2, dv2_du3],\n",
    "                         [dv3_du1, dv3_du2, dv3_du3]])\n",
    "\n",
    "        # dy/du  (2×3)\n",
    "        J_yu = J_yv @ J_vu\n",
    "\n",
    "        return J_yv, J_vu, J_yu\n",
    "\n",
    "\n",
    "# ---------- run ----------\n",
    "x = np.array([1., 1., 1.])\n",
    "model = CompGraph()\n",
    "y = model.forward(*x)\n",
    "J_yv, J_vu, J_yu = model.backward()\n",
    "\n",
    "print(\"Forward output y1, y2 =\", y)\n",
    "print()\n",
    "print(\"J_yv = ∂y/∂v\\n\", J_yv)\n",
    "print()\n",
    "print(\"J_vu = ∂v/∂u\\n\", J_vu)\n",
    "print()\n",
    "print(\"J_yu = ∂y/∂u (analytic)\\n\", J_yu)\n",
    "print()\n",
    "print(\"Chain-rule check: J_yv @ J_vu\\n\", J_yv @ J_vu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4acbcd-b53f-47ea-a6e6-2b282597e7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2-hidden-layer FFNN implemented with NumPy for the OCR dataset.\n",
    "- Switch datasets by changing `dataset_size = 'small'|'medium'|'large'`.\n",
    "- Switch activation with activation_type = 'sigmoid' or 'relu'.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906b27cc-c866-42b6-bfd4-4680b25faf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Configuration / hyperparams\n",
    "# -----------------------------\n",
    "# dataset_size = 'large'   # 'small', 'medium', or 'large'\n",
    "datasets = ['small', 'medium', 'large']\n",
    "\n",
    "# Network architecture\n",
    "H1 = 128   # hidden layer 1 units\n",
    "H2 = 62   # hidden layer 2 units\n",
    "\n",
    "label_map = {0:'a',1:'e',2:'g',3:'i',4:'l',5:'n',6:'o',7:'r',8:'t',9:'u'}\n",
    "\n",
    "# Training hyperparameters\n",
    "epochs = 20\n",
    "batch_size = 64\n",
    "lr = 0.5\n",
    "activation_type = 'sigmoid'  # 'sigmoid' or 'relu'\n",
    "\n",
    "seed = 0\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923a9e4b-9302-4169-a851-f54c15e42849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Utility math & activations\n",
    "# -----------------------------\n",
    "def sigmoid(x):\n",
    "    # Prevent overflow in exp() by clipping extreme values\n",
    "    x = np.clip(x, -60, 60)\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def sigmoid_grad_from_sigma(sig_x):\n",
    "    # If sig_x = σ(x), then σ'(x) = σ(x)*(1-σ(x))\n",
    "    return sig_x * (1.0 - sig_x)\n",
    "\n",
    "def relu(x):\n",
    "    # ReLU(x) = max(0, x) (elementwise)\n",
    "    return np.maximum(0.0, x)\n",
    "\n",
    "def relu_grad(x):\n",
    "    # ReLU'(x) = 1 if x > 0 else 0 (elementwise)\n",
    "    g = np.zeros_like(x)\n",
    "    g[x > 0] = 1.0\n",
    "    return g\n",
    "\n",
    "def softmax(logits):\n",
    "    \"\"\"\n",
    "    Softmax row-wise with numeric stability\n",
    "    For row i: softmax_i_j = exp(logit_i_j - m_i) / sum_k exp(logit_i_k - m_i)\n",
    "    where m_i = max_k logit_i_k\n",
    "    \"\"\"\n",
    "    # shift = logits - m (broadcast)\n",
    "    m = np.max(logits, axis=1, keepdims=True)          # m_i = max_k logits[i,k]   (shape N x 1)\n",
    "    shift = logits - m                                 # shift[i,j] = logits[i,j] - m_i\n",
    "    exp_shift = np.exp(shift)                          # exp_shift[i,j] = exp(shift[i,j])\n",
    "    sum_exp = np.sum(exp_shift, axis=1, keepdims=True) # sum_exp[i] = sum_k exp_shift[i,k]\n",
    "    probs = exp_shift / sum_exp                        # probs[i,j] = exp_shift[i,j] / sum_exp[i]\n",
    "    return probs\n",
    "\n",
    "def cross_entropy_loss(probs, y_onehot):\n",
    "    \"\"\"\n",
    "    L = - (1/N) * sum_{i=1..N} sum_{k=1..K} y_{i,k} * log(probs_{i,k})\n",
    "    We clip probs to avoid log(0).\n",
    "    \"\"\"\n",
    "    N = probs.shape[0]\n",
    "    clipped = np.clip(probs, 1e-12, 1.0)\n",
    "    return -np.sum(y_onehot * np.log(clipped)) / N\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a5894d-5868-451d-98c4-e7f0b08e0fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(train_path, split_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Load {dataset_size}Train.csv and split into train/test internally (no Validation.csv).\n",
    "    Splits each dataset into (1 - split_ratio) train and split_ratio test.\n",
    "    \"\"\"\n",
    "    train_path = os.path.join(f\"./图像分类-dataset/{dataset_size}Train.csv\")\n",
    "    df = pd.read_csv(train_path, header=None)\n",
    "\n",
    "    # separate labels and pixel data\n",
    "    y = df.iloc[:, 0].values.astype(int)  # labels (shape N,)\n",
    "    X = df.iloc[:, 1:].values.astype(np.float32)  # pixels (shape N x 128)\n",
    "\n",
    "    # normalize to [0,1]\n",
    "    if X.max() > 1.0:\n",
    "        X /= 255.0  # elementwise: X[i,j] = X[i,j] / 255\n",
    "\n",
    "    # split into train/test\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=split_ratio, stratify=y, random_state=0\n",
    "    )\n",
    "\n",
    "    # one-hot encode labels for loss (K classes)\n",
    "    enc = OneHotEncoder(sparse_output=False, categories='auto')\n",
    "    y_train_onehot = enc.fit_transform(y_train.reshape(-1, 1))  # shape (N_train, K)\n",
    "    y_val_onehot = enc.transform(y_val.reshape(-1, 1))  # shape (N_val, K)\n",
    "\n",
    "    return X_train, y_train, y_train_onehot, X_val, y_val, y_val_onehot\n",
    "\n",
    "# M = X_train.shape[1]   # input dim (128)\n",
    "# K = y_train_onehot.shape[1]  # number of classes, expected 10\n",
    "\n",
    "def show_samples(X, y, dataset_name):\n",
    "    indices = np.random.choice(len(X), 10, replace=False)\n",
    "    fig, axes = plt.subplots(2,5, figsize=(8,4))\n",
    "    for ax, idx in zip(axes.flatten(), indices):\n",
    "        ax.imshow(1 - X[idx].reshape(16,8), cmap='gray', vmin=0, vmax=1)\n",
    "        ax.set_title(f'{label_map[y[idx]]}')\n",
    "        ax.axis('off')\n",
    "    plt.suptitle(f'{dataset_name.upper()} Dataset — 10 Random Samples')\n",
    "    plt.show()\n",
    "    \n",
    "for name in datasets:\n",
    "    print(f\"\\n===== DATASET: {name.upper()} =====\")\n",
    "    X_train, y_train, y_train_1hot, X_val, y_val, y_val_1hot = load_dataset(train_path, split_ratio=0.2)\n",
    "    show_samples(X_train, y_train, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c518899-cbc6-448c-8046-e3ba0998cd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Helper: choose activation\n",
    "# -----------------------------\n",
    "def activation_forward(z):\n",
    "    if activation_type == 'sigmoid':\n",
    "        return sigmoid(z)        # a = σ(z)\n",
    "    elif activation_type == 'relu':\n",
    "        return relu(z)           # a = max(0, z)\n",
    "    else:\n",
    "        raise ValueError(\"activation_type must be 'sigmoid' or 'relu'\")\n",
    "\n",
    "def activation_backward(z, a):\n",
    "    \"\"\"\n",
    "    Return derivative φ'(z).\n",
    "    For sigmoid: we pass a = σ(z) and compute σ'(z) = a*(1-a).\n",
    "    For ReLU: need to compute indicator(z>0).\n",
    "    \"\"\"\n",
    "    if activation_type == 'sigmoid':\n",
    "        return sigmoid_grad_from_sigma(a)  # φ'(z) elementwise\n",
    "    elif activation_type == 'relu':\n",
    "        return relu_grad(z)\n",
    "    else:\n",
    "        raise ValueError(\"activation_type must be 'sigmoid' or 'relu'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c11fe4c-3ad9-425d-923c-3953a6187129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Training loop\n",
    "# -----------------------------\n",
    "def train_model(X_train, y_train, y_train_1hot,\n",
    "                X_test, y_test, y_test_1hot,\n",
    "                learning_rate=0.5, epochs=200, batch_size=64,\n",
    "                activation_function='relu'):\n",
    "    num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
    "    loss_history = []\n",
    "\n",
    "    # -----------------------------\n",
    "    # Parameter initialization\n",
    "    # -----------------------------\n",
    "    # Xavier / Glorot-like initialization: normal(0, sqrt(2/(n_in + n_out)))\n",
    "    W1 = np.random.normal(0.0, np.sqrt(2.0 / (M + H1)), size=(M, H1))   # shape (M, H1)\n",
    "    b1 = np.zeros((1, H1))                                               # shape (1, H1)\n",
    "    \n",
    "    W2 = np.random.normal(0.0, np.sqrt(2.0 / (H1 + H2)), size=(H1, H2))  # shape (H1, H2)\n",
    "    b2 = np.zeros((1, H2))                                               # shape (1, H2)\n",
    "    \n",
    "    W3 = np.random.normal(0.0, np.sqrt(2.0 / (H2 + K)), size=(H2, K))    # shape (H2, K)\n",
    "    b3 = np.zeros((1, K)) \n",
    "\n",
    "    # alias learning_rate for consistency with original lr variable\n",
    "    lr = learning_rate\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # shuffle dataset each epoch\n",
    "        X_shuf, y_shuf, yoh_shuf = shuffle(X_train, y_train, y_train_1hot, random_state=epoch)\n",
    "        epoch_loss_sum = 0.0\n",
    "\n",
    "        for b in range(num_batches):\n",
    "            start = b * batch_size\n",
    "            end = min((b+1) * batch_size, X_shuf.shape[0])\n",
    "            Xb = X_shuf[start:end]        # shape (N, M)\n",
    "            yb = yoh_shuf[start:end]      # shape (N, K)\n",
    "            N = Xb.shape[0]\n",
    "\n",
    "            # --------------------------\n",
    "            # Forward propagation\n",
    "            # --------------------------\n",
    "            z1 = Xb.dot(W1) + b1          # shape (N, H1)\n",
    "            a1 = activation_forward(z1)   # shape (N, H1)\n",
    "            z2 = a1.dot(W2) + b2          # shape (N, H2)\n",
    "            a2 = activation_forward(z2)   # shape (N, H2)\n",
    "            z3 = a2.dot(W3) + b3          # shape (N, K)\n",
    "            probs = softmax(z3)           # shape (N, K)\n",
    "\n",
    "            # loss (average cross-entropy over batch)\n",
    "            loss = cross_entropy_loss(probs, yb)\n",
    "            epoch_loss_sum += loss * N\n",
    "\n",
    "            # --------------------------\n",
    "            # Backward propagation\n",
    "            # --------------------------\n",
    "            delta3 = (probs - yb) / N     # shape (N, K)\n",
    "            dW3 = a2.T.dot(delta3)        # shape (H2, K)\n",
    "            db3 = np.sum(delta3, axis=0, keepdims=True)  # shape (1, K)\n",
    "\n",
    "            temp2 = delta3.dot(W3.T)      # shape (N, H2)\n",
    "            delta2 = temp2 * activation_backward(z2, a2)  # shape (N, H2)\n",
    "            dW2 = a1.T.dot(delta2)        # shape (H1, H2)\n",
    "            db2 = np.sum(delta2, axis=0, keepdims=True)  # shape (1, H2)\n",
    "\n",
    "            temp1 = delta2.dot(W2.T)      # shape (N, H1)\n",
    "            delta1 = temp1 * activation_backward(z1, a1)  # shape (N, H1)\n",
    "            dW1 = Xb.T.dot(delta1)        # shape (M, H1)\n",
    "            db1 = np.sum(delta1, axis=0, keepdims=True)  # shape (1, H1)\n",
    "\n",
    "            # --------------------------\n",
    "            # Parameter update (SGD)\n",
    "            # --------------------------\n",
    "            W3 -= lr * dW3; b3 -= lr * db3\n",
    "            W2 -= lr * dW2; b2 -= lr * db2\n",
    "            W1 -= lr * dW1; b1 -= lr * db1\n",
    "\n",
    "        # --------------------------\n",
    "        # End of epoch: compute loss and accuracy\n",
    "        # --------------------------\n",
    "        epoch_loss = epoch_loss_sum / X_train.shape[0]\n",
    "        loss_history.append(epoch_loss)\n",
    "\n",
    "        # Validation / test forward pass\n",
    "        z1_v = X_test.dot(W1) + b1\n",
    "        a1_v = activation_forward(z1_v)\n",
    "        z2_v = a1_v.dot(W2) + b2\n",
    "        a2_v = activation_forward(z2_v)\n",
    "        z3_v = a2_v.dot(W3) + b3\n",
    "        probs_v = softmax(z3_v)\n",
    "        preds_v = np.argmax(probs_v, axis=1)\n",
    "        val_acc = np.mean(preds_v == y_test)\n",
    "\n",
    "        if (epoch % 20 == 0) or (epoch == 1):\n",
    "            print(f\"Epoch {epoch:3d}/{epochs} — Train Loss: {epoch_loss:.4f}, Test Acc: {val_acc*100:.2f}%\")\n",
    "\n",
    "    np.savez(os.path.join(f\"./ffnn_{dataset_size}_{activation_type}_params.npz\"),\n",
    "         W1=W1, b1=b1, W2=W2, b2=b2, W3=W3, b3=b3)\n",
    "    print(\"\\nFinished training. Parameters saved to disk.\")\n",
    "    # Return everything needed for later analysis\n",
    "    return loss_history, val_acc, preds_v, probs_v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035401ec-da61-4d26-972f-91e6f8aaee9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# Plot Loss Function Changes for Small, Medium, and Large Datasets\n",
    "# ==============================================================\n",
    "\n",
    "loss_curves = {}\n",
    "accuracies = {}\n",
    "\n",
    "for dataset_size in ['small', 'medium', 'large']:\n",
    "    print(f\"\\n===== TRAINING ON {dataset_size.upper()} DATASET =====\")\n",
    "\n",
    "    X_train, y_train, y_train_1hot, X_val, y_val, y_val_1hot = load_dataset(train_path, split_ratio=0.2)\n",
    "\n",
    "    # Recalculate input/output dimensions for this dataset\n",
    "    M = X_train.shape[1]\n",
    "    K = y_train_1hot.shape[1]\n",
    "\n",
    "    # Train the model using your existing train_model() function\n",
    "    loss_history, val_acc, preds, probs = train_model(\n",
    "        X_train, y_train, y_train_1hot,\n",
    "        X_val, y_val, y_val_1hot,\n",
    "        learning_rate=lr, epochs=epochs, batch_size=batch_size,\n",
    "        activation_function=activation_type\n",
    "    )\n",
    "\n",
    "    loss_curves[dataset_size] = loss_history\n",
    "    accuracies[dataset_size] = val_acc\n",
    "\n",
    "    print(f\"Final Accuracy on {dataset_size.upper()}: {val_acc*100:.2f}%\")\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Plot all three loss curves on the same graph for comparison\n",
    "# --------------------------------------------------------------\n",
    "plt.figure(figsize=(8,5))\n",
    "for name, losses in loss_curves.items():\n",
    "    plt.plot(range(1, len(losses)+1), losses, label=f\"{name} dataset\")\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Training Loss (Cross-Entropy)\")\n",
    "plt.title(\"Loss Function Change During Training (NumPy 2-Layer FFNN)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Print summary of final accuracies\n",
    "# --------------------------------------------------------------\n",
    "print(\"\\nSummary of Final Accuracies:\")\n",
    "for ds, acc in accuracies.items():\n",
    "    print(f\"{ds.capitalize():<10}: {acc*100:.2f}%\")\n",
    "\n",
    "# Save final weights if desired\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001f8c11-569a-460f-ad18-33e2a609a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# Show 5 test samples (image + True/Predicted labels) per dataset\n",
    "# ==============================================================\n",
    "\n",
    "for dataset_size in ['small', 'medium', 'large']:\n",
    "    print(f\"\\n===== {dataset_size.upper()} DATASET — 5 SAMPLE PREDICTIONS =====\")\n",
    "\n",
    "    # Load dataset (split internally into train/test)\n",
    "    train_path = os.path.join(f\"./图像分类-dataset/{dataset_size}Train.csv\")\n",
    "    X_train, y_train, y_train_1hot, X_val, y_val, y_val_1hot = load_dataset(train_path, split_ratio=0.2)\n",
    "\n",
    "    # Infer input/output dimensions\n",
    "    M = X_train.shape[1]\n",
    "    K = y_train_1hot.shape[1]\n",
    "\n",
    "    # Train model\n",
    "    loss_history, val_acc, preds_v, probs_v = train_model(\n",
    "        X_train, y_train, y_train_1hot,\n",
    "        X_val, y_val, y_val_1hot,\n",
    "        learning_rate=lr, epochs=epochs, batch_size=batch_size,\n",
    "        activation_function=activation_type\n",
    "    )\n",
    "\n",
    "    # Pick 5 random test samples\n",
    "    sample_indices = np.random.choice(len(X_val), 5, replace=False)\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(8, 2))\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        idx = sample_indices[i]\n",
    "        image = X_val[idx].reshape(16, 8)\n",
    "        true_label = label_map[y_val[idx]]\n",
    "        pred_label = label_map[preds_v[idx]]\n",
    "\n",
    "        ax.imshow(image, cmap='gray', vmin=0, vmax=1)\n",
    "        ax.set_title(f\"True Label:{true_label}\\nPrediction Label:{pred_label}\", fontsize=8)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.suptitle(f\"{dataset_size.upper()} Dataset — 5 Example Predictions\", fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146d581f-2ccf-4303-be91-1956f908ee49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# PyTorch Implementation of the same FNN (Letter Classification)\n",
    "# ==============================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 1. Define the PyTorch Feedforward Neural Network\n",
    "# --------------------------------------------------------------\n",
    "class FNNClassifierTorch(nn.Module):\n",
    "    def __init__(self, input_dim, hidden1=128, hidden2=62, output_dim=10):\n",
    "        super(FNNClassifierTorch, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        x = self.fc3(x)  # logits\n",
    "        return x\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 2. Helper: convert NumPy → Torch tensors\n",
    "# --------------------------------------------------------------\n",
    "def to_torch(X, y):\n",
    "    X_t = torch.tensor(X, dtype=torch.float32)\n",
    "    y_t = torch.tensor(y, dtype=torch.long)\n",
    "    return X_t, y_t\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 3. Train + Evaluate the model\n",
    "# --------------------------------------------------------------\n",
    "def train_torch_model(X_train, y_train, X_val, y_val, learning_rate=0.5, epochs=20, batch_size=64):\n",
    "    input_dim = X_train.shape[1]\n",
    "    output_dim = len(np.unique(y_train))\n",
    "\n",
    "    model = FNNClassifierTorch(input_dim, H1, H2, output_dim)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    X_train_t, y_train_t = to_torch(X_train, y_train)\n",
    "    X_val_t, y_val_t = to_torch(X_val, y_val)\n",
    "\n",
    "    num_batches = int(np.ceil(len(X_train_t) / batch_size))\n",
    "    loss_history = []\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        # Mini-batch training\n",
    "        for i in range(num_batches):\n",
    "            start, end = i * batch_size, min((i + 1) * batch_size, len(X_train_t))\n",
    "            xb, yb = X_train_t[start:end], y_train_t[start:end]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * len(xb)\n",
    "\n",
    "        # Compute average epoch loss\n",
    "        avg_loss = epoch_loss / len(X_train_t)\n",
    "        loss_history.append(avg_loss)\n",
    "\n",
    "        # Validation accuracy\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits_val = model(X_val_t)\n",
    "            preds_val = torch.argmax(logits_val, dim=1)\n",
    "            acc = (preds_val == y_val_t).float().mean().item()\n",
    "\n",
    "        if (epoch == 1) or (epoch % 5 == 0):\n",
    "            print(f\"Epoch {epoch:3d}/{epochs} — Train Loss: {avg_loss:.4f}, Val Acc: {acc*100:.2f}%\")\n",
    "\n",
    "    return model, loss_history, acc, preds_val\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 4. Train and evaluate on small / medium / large datasets\n",
    "# --------------------------------------------------------------\n",
    "torch_loss_curves = {}\n",
    "torch_accuracies = {}\n",
    "\n",
    "for dataset_size in ['small', 'medium', 'large']:\n",
    "    print(f\"\\n===== PyTorch TRAINING ON {dataset_size.upper()} DATASET =====\")\n",
    "\n",
    "    train_path = os.path.join(f\"./图像分类-dataset/{dataset_size}Train.csv\")\n",
    "    X_train, y_train, y_train_1hot, X_val, y_val, y_val_1hot = load_dataset(train_path, split_ratio=0.2)\n",
    "\n",
    "    model, loss_hist, val_acc, preds_val = train_torch_model(\n",
    "        X_train, y_train, X_val, y_val,\n",
    "        learning_rate=lr, epochs=epochs, batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    torch_loss_curves[dataset_size] = loss_hist\n",
    "    torch_accuracies[dataset_size] = val_acc\n",
    "\n",
    "    print(f\"Final Accuracy ({dataset_size.upper()}): {val_acc*100:.2f}%\")\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Display 5 random test samples (True vs Predicted)\n",
    "    # ----------------------------------------------------------\n",
    "    sample_indices = np.random.choice(len(X_val), 5, replace=False)\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(8, 2))\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        idx = sample_indices[i]\n",
    "        img = X_val[idx].reshape(16, 8)\n",
    "        true_label = label_map[y_val[idx]]\n",
    "        pred_label = label_map[preds_val[idx].item()]\n",
    "\n",
    "        ax.imshow(img, cmap='gray', vmin=0, vmax=1)\n",
    "        ax.set_title(f\"True:{true_label}\\nPred:{pred_label}\", fontsize=8)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.suptitle(f\"{dataset_size.upper()} Dataset — 5 PyTorch Predictions\", fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 5. Plot PyTorch Loss Curves for Comparison\n",
    "# --------------------------------------------------------------\n",
    "plt.figure(figsize=(8,5))\n",
    "for name, losses in torch_loss_curves.items():\n",
    "    plt.plot(range(1, len(losses)+1), losses, label=f\"{name} dataset\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Training Loss (Cross-Entropy)\")\n",
    "plt.title(\"Loss Function Change During Training (PyTorch 2-Layer FFNN)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 6. Print Summary\n",
    "# --------------------------------------------------------------\n",
    "print(\"\\nSummary of Final Accuracies (PyTorch):\")\n",
    "for ds, acc in torch_accuracies.items():\n",
    "    print(f\"{ds.capitalize():<10}: {acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb1ea12-42c0-47c8-aa89-4c24fb8c18fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3163882f-0504-4ee4-9994-5cebc49f57f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9eb90b3-d09d-4509-a994-b4ffd9d16a12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3126c0-fbca-4f9f-951f-3d45ce43c4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# FFNN Regression (NumPy + PyTorch)\n",
    "# Based on 回归-dataset\n",
    "# ==============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# --------------------------\n",
    "# 1) Load dataset\n",
    "# --------------------------\n",
    "train_df = pd.read_csv(\"./回归-dataset/data_train.csv\", header=None, skiprows=1)\n",
    "valid_df = pd.read_csv(\"./回归-dataset/data_valid.csv\", header=None, skiprows=1)\n",
    "test_df  = pd.read_csv(\"./回归-dataset/data_test.csv\",  header=None, skiprows=1)\n",
    "\n",
    "X_all = train_df.iloc[:, 0].to_numpy(float).reshape(-1, 1)\n",
    "Y_all = train_df.iloc[:, 1].to_numpy(float).reshape(-1, 1)\n",
    "perm = np.random.permutation(len(X_all))\n",
    "X_all, Y_all = X_all[perm], Y_all[perm]\n",
    "\n",
    "X_train, Y_train = X_all[:1800], Y_all[:1800]\n",
    "X_test_ID, Y_test_ID = X_all[1800:], Y_all[1800:]\n",
    "X_test_OOD = np.concatenate([\n",
    "    valid_df.iloc[:, 0].to_numpy(float).reshape(-1, 1),\n",
    "    test_df.iloc[:, 0].to_numpy(float).reshape(-1, 1)\n",
    "], axis=0)\n",
    "Y_test_OOD = np.concatenate([\n",
    "    valid_df.iloc[:, 1].to_numpy(float).reshape(-1, 1),\n",
    "    test_df.iloc[:, 1].to_numpy(float).reshape(-1, 1)\n",
    "], axis=0)\n",
    "\n",
    "# ==============================================================\n",
    "# 2. Visualize In-Distribution and Out-of-Distribution Sets\n",
    "# ==============================================================\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(X_train, Y_train, s=10, c='blue', label=\"Train Data (ID)\")\n",
    "plt.scatter(X_test_ID, Y_test_ID, s=10, c='gold', label=\"Test Data (ID)\")\n",
    "plt.scatter(X_test_OOD, Y_test_OOD, s=10, c='limegreen', label=\"Test Data (OOD)\")\n",
    "plt.axvline(x=2.0, color='black', linestyle='--', label=\"OOD Boundary\")\n",
    "plt.legend()\n",
    "plt.title(\"Dataset Distribution: In-Distribution (ID) vs Out-of-Distribution (OOD)\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"| ID-Test:\", X_test_ID.shape, \"| OOD-Test:\", X_test_OOD.shape)\n",
    "\n",
    "# --------------------------\n",
    "# 2) Normalization\n",
    "# --------------------------\n",
    "def fit_norm(X, Y):\n",
    "    return X.mean(0, keepdims=True), X.std(0, keepdims=True), Y.mean(0, keepdims=True), Y.std(0, keepdims=True)\n",
    "def apply_norm(X, Y, X_mu, X_sd, Y_mu, Y_sd):\n",
    "    return ( (X - X_mu) / (X_sd + 1e-8), (Y - Y_mu) / (Y_sd + 1e-8) )\n",
    "def denorm_Y(Yn, Y_mu, Y_sd): return Yn * (Y_sd + 1e-8) + Y_mu\n",
    "\n",
    "X_mu, X_sd, Y_mu, Y_sd = fit_norm(X_train, Y_train)\n",
    "Xtr_n, Ytr_n = apply_norm(X_train, Y_train, X_mu, X_sd, Y_mu, Y_sd)\n",
    "Xid_n, Yid_n = apply_norm(X_test_ID, Y_test_ID, X_mu, X_sd, Y_mu, Y_sd)\n",
    "Xod_n, Yod_n = apply_norm(X_test_OOD, Y_test_OOD, X_mu, X_sd, Y_mu, Y_sd)\n",
    "\n",
    "# ==============================================================\n",
    "# NumPy Implementation\n",
    "# ==============================================================\n",
    "\n",
    "def sigmoid(x): x = np.clip(x, -60, 60); return 1.0 / (1.0 + np.exp(-x))\n",
    "def tanh(x): return np.tanh(x)\n",
    "def relu(x): return np.maximum(0, x)\n",
    "def sin(x): return np.sin(x)\n",
    "def batch_norm(z, eps=1e-8): mu=z.mean(0,keepdims=True); sd=z.std(0,keepdims=True); return (z-mu)/(sd+eps)\n",
    "def mse(y, t): return 0.5*np.mean((y-t)**2)\n",
    "def rmse_denorm(yh, yt, Y_mu, Y_sd): return np.sqrt(np.mean((denorm_Y(yh,Y_mu,Y_sd)-denorm_Y(yt,Y_mu,Y_sd))**2))\n",
    "\n",
    "acts = {'sigmoid':sigmoid, 'tanh':tanh, 'relu':relu, 'sin':sin}\n",
    "acts_deriv = {\n",
    "    'sigmoid':lambda a: a*(1-a),\n",
    "    'tanh':lambda a: 1-a**2,\n",
    "    'relu':lambda a: (a>0).astype(float),\n",
    "    'sin':lambda a: np.cos(a)\n",
    "}\n",
    "\n",
    "def train_numpy(Xtr, Ytr, Xid, Yid, Xod, Yod, act_name='sigmoid',\n",
    "                hidden1=64, hidden2=32, lr=0.02, epochs=20000, bn=True):\n",
    "    np.random.seed(42)\n",
    "    f, df = acts[act_name], acts_deriv[act_name]\n",
    "    N,M = Xtr.shape\n",
    "    W1 = np.random.randn(M,hidden1)*np.sqrt(2/(M+hidden1)); b1=np.zeros((1,hidden1))\n",
    "    W2 = np.random.randn(hidden1,hidden2)*np.sqrt(2/(hidden1+hidden2)); b2=np.zeros((1,hidden2))\n",
    "    W3 = np.random.randn(hidden2,1)*np.sqrt(2/(hidden2+1)); b3=np.zeros((1,1))\n",
    "\n",
    "    def forward(X):\n",
    "        z1 = X@W1+b1; a1=f(batch_norm(z1) if bn else z1)\n",
    "        z2 = a1@W2+b2; a2=f(batch_norm(z2) if bn else z2)\n",
    "        z3 = a2@W3+b3; return z1,a1,z2,a2,z3\n",
    "\n",
    "    snap={0:(W1.copy(),b1.copy(),W2.copy(),b2.copy(),W3.copy(),b3.copy())}\n",
    "    tr_hist,id_hist,od_hist=[],[],[]\n",
    "\n",
    "    for ep in range(1,epochs+1):\n",
    "        z1,a1,z2,a2,yhat=forward(Xtr)\n",
    "        dY=(yhat-Ytr)/N\n",
    "        dW3=a2.T@dY; db3=dY.sum(0,keepdims=True)\n",
    "        d2=(dY@W3.T)*df(a2); dW2=a1.T@d2; db2=d2.sum(0,keepdims=True)\n",
    "        d1=(d2@W2.T)*df(a1); dW1=Xtr.T@d1; db1=d1.sum(0,keepdims=True)\n",
    "        W3-=lr*dW3; b3-=lr*db3; W2-=lr*dW2; b2-=lr*db2; W1-=lr*dW1; b1-=lr*db1\n",
    "\n",
    "        if ep%1000==0:\n",
    "            fwd=lambda X: forward(X)[-1]\n",
    "            tr,idt,ood = mse(fwd(Xtr),Ytr), mse(fwd(Xid),Yid), mse(fwd(Xod),Yod)\n",
    "            tr_hist.append(tr); id_hist.append(idt); od_hist.append(ood)\n",
    "            print(f\"[{act_name}] Epoch {ep:5d} | MSE Train={tr:.4f} ID={idt:.4f} OOD={ood:.4f}\")\n",
    "\n",
    "        if ep in (0,100,200): snap[ep]=(W1.copy(),b1.copy(),W2.copy(),b2.copy(),W3.copy(),b3.copy())\n",
    "    snap['final']=(W1.copy(),b1.copy(),W2.copy(),b2.copy(),W3.copy(),b3.copy())\n",
    "    return snap,tr_hist,id_hist,od_hist,lambda X:forward(X)[-1]\n",
    "\n",
    "def predict_rawY(X,params,act_name,bn=True):\n",
    "    f=acts[act_name]; W1,b1,W2,b2,W3,b3=params\n",
    "    Xn=(X-X_mu)/(X_sd+1e-8)\n",
    "    z1=Xn@W1+b1; a1=f(batch_norm(z1) if bn else z1)\n",
    "    z2=a1@W2+b2; a2=f(batch_norm(z2) if bn else z2)\n",
    "    y=a2@W3+b3\n",
    "    return denorm_Y(y,Y_mu,Y_sd)\n",
    "    \n",
    "name_map = {\n",
    "    \"relu\": \"Linear\",\n",
    "    \"sigmoid\": \"Quadratic\",\n",
    "    \"tanh\": \"Tanh\",\n",
    "    \"sin\": \"Periodic\"\n",
    "}\n",
    "\n",
    "def plot_panels(act_name,snap,tr_hist,id_hist,od_hist):\n",
    "    fig,ax=plt.subplots(1,5,figsize=(18,3.6)); plt.subplots_adjust(wspace=0.35)\n",
    "    xg=np.linspace(-4,6,400).reshape(-1,1)\n",
    "    y_min,y_max=Y_train.min(),Y_train.max(); margin=0.05*(y_max-y_min)\n",
    "    stages=[(\"Init\",snap[0]),(\"Epoch 100\",snap[100]),(\"Epoch 200\",snap[200]),(\"Final\",snap['final'])]\n",
    "    for i,(t,p) in enumerate(stages):\n",
    "        ax[i].set_title(f\"{act_name.upper()} - {t}\")\n",
    "        ax[i].set_xlim(-4,6); ax[i].set_ylim(y_min-margin,y_max+margin)\n",
    "        ax[i].axvspan(2,6,color='gray',alpha=0.15); ax[i].axvline(x=2,color='black',ls='--')\n",
    "        ax[i].scatter(X_train,Y_train,s=10,c='blue',label='Train')\n",
    "        ax[i].scatter(X_test_ID,Y_test_ID,s=10,c='gold',label='ID')\n",
    "        ax[i].scatter(X_test_OOD,Y_test_OOD,s=10,c='lime',label='OOD')\n",
    "        ax[i].plot(xg,predict_rawY(xg,p,act_name,bn=True),c='red',lw=2)\n",
    "        if i==0: ax[i].legend(fontsize=8)\n",
    "    e=np.arange(len(tr_hist))*1000\n",
    "    ax[-1].plot(e,tr_hist,label='Train',c='blue'); ax[-1].plot(e,id_hist,label='ID',c='orange'); ax[-1].plot(e,od_hist,label='OOD',c='green')\n",
    "    ax[-1].set_title(\"Loss (MSE)\"); ax[-1].set_xlabel(\"Epochs\"); ax[-1].grid(True); ax[-1].legend(fontsize=8)\n",
    "    exp_name = name_map[act]\n",
    "    plt.suptitle(f\"NumPy FFNN Regression ({exp_name})\",y=1.03)\n",
    "    plt.show()\n",
    "\n",
    "for act in ['sigmoid','tanh','relu','sin']:\n",
    "    snap,tr_hist,id_hist,od_hist,predict = train_numpy(Xtr_n,Ytr_n,Xid_n,Yid_n,Xod_n,Yod_n,act_name=act,epochs=20000)\n",
    "    plot_panels(act,snap,tr_hist,id_hist,od_hist)\n",
    "    print(f\"{act.upper()} RMSE (Train/ID/OOD):\",\n",
    "          rmse_denorm(predict(Xtr_n),Ytr_n,Y_mu,Y_sd),\n",
    "          rmse_denorm(predict(Xid_n),Yid_n,Y_mu,Y_sd),\n",
    "          rmse_denorm(predict(Xod_n),Yod_n,Y_mu,Y_sd))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6768d6b3-ea15-4c66-b9a2-2725c78543dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# PyTorch FFNN Regression (Experiment 3.3 Style)\n",
    "# ==============================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "# ==============================================================\n",
    "# 1. Load Datasets\n",
    "# ==============================================================\n",
    "train_df = pd.read_csv(\"./回归-dataset/data_train.csv\", header=None, skiprows=1)\n",
    "valid_df = pd.read_csv(\"./回归-dataset/data_valid.csv\", header=None, skiprows=1)\n",
    "test_df  = pd.read_csv(\"./回归-dataset/data_test.csv\", header=None, skiprows=1)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X_all = train_df.iloc[:, 0].to_numpy(dtype=float).reshape(-1, 1)\n",
    "Y_all = train_df.iloc[:, 1].to_numpy(dtype=float).reshape(-1, 1)\n",
    "\n",
    "# Random shuffle and split\n",
    "np.random.seed(42)\n",
    "idx = np.random.permutation(len(X_all))\n",
    "X_all, Y_all = X_all[idx], Y_all[idx]\n",
    "X_train, Y_train = X_all[:1800], Y_all[:1800]\n",
    "X_test_ID, Y_test_ID = X_all[1800:], Y_all[1800:]\n",
    "\n",
    "# OOD test set\n",
    "X_test_OOD = np.concatenate([\n",
    "    valid_df.iloc[:, 0].values.reshape(-1, 1),\n",
    "    test_df.iloc[:, 0].values.reshape(-1, 1)\n",
    "])\n",
    "Y_test_OOD = np.concatenate([\n",
    "    valid_df.iloc[:, 1].values.reshape(-1, 1),\n",
    "    test_df.iloc[:, 1].values.reshape(-1, 1)\n",
    "])\n",
    "\n",
    "# Normalize input/output\n",
    "X_mu, X_sd = X_train.mean(), X_train.std()\n",
    "Y_mu, Y_sd = Y_train.mean(), Y_train.std()\n",
    "\n",
    "def norm_X(x): return (x - X_mu) / (X_sd + 1e-8)\n",
    "def norm_Y(y): return (y - Y_mu) / (Y_sd + 1e-8)\n",
    "def denorm_Y(y): return y * (Y_sd + 1e-8) + Y_mu\n",
    "\n",
    "X_train_t = torch.tensor(norm_X(X_train), dtype=torch.float32)\n",
    "Y_train_t = torch.tensor(norm_Y(Y_train), dtype=torch.float32)\n",
    "X_ID_t = torch.tensor(norm_X(X_test_ID), dtype=torch.float32)\n",
    "Y_ID_t = torch.tensor(norm_Y(Y_test_ID), dtype=torch.float32)\n",
    "X_OOD_t = torch.tensor(norm_X(X_test_OOD), dtype=torch.float32)\n",
    "Y_OOD_t = torch.tensor(norm_Y(Y_test_OOD), dtype=torch.float32)\n",
    "\n",
    "# ==============================================================\n",
    "# 2. Define Model Architectures for Various Activations\n",
    "# ==============================================================\n",
    "class Sin(nn.Module):\n",
    "    def forward(self, x): return torch.sin(x)\n",
    "\n",
    "def get_activation(name):\n",
    "    if name == 'sigmoid': return nn.Sigmoid()\n",
    "    if name == 'tanh': return nn.Tanh()\n",
    "    if name == 'relu': return nn.ReLU()\n",
    "    if name == 'sin': return Sin()\n",
    "    raise ValueError(\"Unknown activation type\")\n",
    "\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, act_type='sigmoid'):\n",
    "        super().__init__()\n",
    "        act = get_activation(act_type)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, 64),\n",
    "            act,\n",
    "            nn.Linear(64, 32),\n",
    "            act,\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# ==============================================================\n",
    "# 3. Training Function (with Model Snapshots)\n",
    "# ==============================================================\n",
    "def train_torch(act='sigmoid', lr=1e-3, epochs=20000):\n",
    "    model = FFNN(act)\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    train_hist, id_hist, od_hist = [], [], []\n",
    "    snapshots = {}\n",
    "\n",
    "    for epoch in range(epochs + 1):\n",
    "        model.train()\n",
    "        opt.zero_grad()\n",
    "        y_pred = model(X_train_t)\n",
    "        loss = loss_fn(y_pred, Y_train_t)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        if epoch % 1000 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                id_pred = model(X_ID_t)\n",
    "                od_pred = model(X_OOD_t)\n",
    "                id_loss = loss_fn(id_pred, Y_ID_t).item()\n",
    "                od_loss = loss_fn(od_pred, Y_OOD_t).item()\n",
    "\n",
    "            train_hist.append(loss.item())\n",
    "            id_hist.append(id_loss)\n",
    "            od_hist.append(od_loss)\n",
    "            print(f\"Epoch {epoch:5d}: Train={loss.item():.4f}, ID={id_loss:.4f}, OOD={od_loss:.4f}\")\n",
    "\n",
    "            # Store snapshots at key epochs\n",
    "            if epoch in [0, 1000, 5000, 20000]:\n",
    "                snapshots[epoch] = copy.deepcopy(model)\n",
    "\n",
    "    return model, snapshots, train_hist, id_hist, od_hist\n",
    "\n",
    "# ==============================================================\n",
    "# 4. Visualization Function (4 Regression Stages + Loss)\n",
    "# ==============================================================\n",
    "def plot_torch_regression_stages(model_snapshots, act, train_hist, id_hist, od_hist):\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(18, 3.5))\n",
    "    plt.subplots_adjust(wspace=0.3)\n",
    "\n",
    "    x_grid = torch.linspace(-4, 6, 400).view(-1, 1)\n",
    "    X_mu_t = torch.tensor(X_mu, dtype=torch.float32)\n",
    "    X_sd_t = torch.tensor(X_sd, dtype=torch.float32)\n",
    "\n",
    "    colors = {\"train\": \"blue\", \"id\": \"gold\", \"ood\": \"lime\"}\n",
    "    y_lim = [0, 25]\n",
    "\n",
    "    for i, (epoch, snapshot) in enumerate(model_snapshots.items()):\n",
    "        ax = axes[i]\n",
    "        ax.set_title(f\"Epoch {epoch}\" if epoch > 0 else \"Initial Model\")\n",
    "        ax.set_xlim(-4, 6)\n",
    "        ax.set_ylim(y_lim)\n",
    "        ax.set_xlabel(\"X\")\n",
    "        if i == 0:\n",
    "            ax.set_ylabel(\"Y\")\n",
    "\n",
    "        ax.axvspan(2, 6, color='gray', alpha=0.15)\n",
    "        ax.axvline(x=2, color='black', linestyle='--')\n",
    "\n",
    "        ax.scatter(X_train, Y_train, s=10, c=colors[\"train\"], label=\"Train\")\n",
    "        ax.scatter(X_test_ID, Y_test_ID, s=10, c=colors[\"id\"], label=\"ID Test\")\n",
    "        ax.scatter(X_test_OOD, Y_test_OOD, s=10, c=colors[\"ood\"], label=\"OOD Test\")\n",
    "\n",
    "        model = snapshot.eval()\n",
    "        with torch.no_grad():\n",
    "            yg = model((x_grid - X_mu_t) / (X_sd_t + 1e-8))\n",
    "        yg = denorm_Y(yg.numpy())\n",
    "\n",
    "        ax.plot(x_grid, yg, color='red', lw=2, label='FFNN Prediction')\n",
    "        if i == 0:\n",
    "            ax.legend(fontsize=8)\n",
    "\n",
    "    # Loss Curve\n",
    "    ax = axes[-1]\n",
    "    epochs = np.arange(len(train_hist)) * 1000\n",
    "    ax.plot(epochs, train_hist, label='Train Loss', color='blue')\n",
    "    ax.plot(epochs, id_hist, label='ID Loss', color='orange')\n",
    "    ax.plot(epochs, od_hist, label='OOD Loss', color='green')\n",
    "    ax.set_title(\"Loss Curves\")\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.set_ylabel(\"Loss (MSE)\")\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True)\n",
    "\n",
    "    fig.suptitle(f\"PyTorch FFNN Regression ({act})\", fontsize=12, y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "# ==============================================================\n",
    "# 5. Run Experiments for All Activations\n",
    "# ==============================================================\n",
    "for act in ['sigmoid', 'tanh', 'relu', 'sin']:\n",
    "    exp_name = name_map[act]\n",
    "    print(f\"\\n===== Training PyTorch FFNN ({exp_name.upper()}) =====\")\n",
    "    model, snapshots, tr_hist, id_hist, od_hist = train_torch(act)\n",
    "    plot_torch_regression_stages(snapshots, act, tr_hist, id_hist, od_hist)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
